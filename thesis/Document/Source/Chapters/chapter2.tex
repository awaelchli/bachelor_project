\chapter{Capturing a Light Field}

\section{The Plenoptic Function and the Light Field}

The plenoptic function, as introduced by~\cite{AdelsonBergen}, is a 7D function that describes the intensity of light for every frequency, along every light ray in space, at any time. 
It is defined as
\begin{align*}
	P \colon \mathbb{R}^3 \times \left[0, 2 \pi \right) \times \left[ 0, \pi \right] \times \mathbb{R}^2 & \to \mathbb{R}^+ \\
	\left(x, y, z, \theta, \phi, t, \lambda \right) & \mapsto P\left(x, y, z, \theta, \phi, t, \lambda \right), 
\end{align*}
where the parameters $\left(x, y, z\right)$ are the coordinates of a point in 3D space and the angles $\left(\theta, \phi \right)$ describe the direction of an incoming light ray at time $t$. 
The light's intensity is given for every wavelength $\lambda$ and thus, the plenoptic function not only captures the visible frequency spectrum but all electromagnetic waves. 
A commonly used measure for light is the radiance, which is obtained from P by integrating over all wavelengths: 
$R\left(x, y, z, \theta, \phi, t\right) = \int_{\mathbb{R}} \! P\left(x, y, z, \theta, \phi, t, \lambda \right) \, \mathrm{d} \lambda$.

In practice, it is impossible to acquire all the data needed to model the 7D plenoptic function and hence it is reasonable to consider only a subset of the parameters. 
Dropping the time parameter $t$ in $R\left( x, y, z, \theta, \phi, t \right) $ yields a 5D function for the radiance in a static scene. 
As described by~\cite{LightFieldRendering}, this five dimensional representation can further be reduced to four dimensions in the following way. 
The radiance along a line is constant in free space and so, the 5D plenoptic function holds redundant information for the points on this line. 
Ignoring this redundancy leads to the equivalent 4D parameterization of the ray space. 
\cite{LightFieldRendering} propose a parameterization by two parallel planes, as seen in figure~\ref{fig:LightFieldParametrization}, where the coordinates of the lines (rays) are given by the intersections with the two planes.
The \textbf{4D light field} $L(u, v, s, t)$ is therefore defined as the radiance along the line intersecting the two planes at coordinates $(u, v)$ and $(s, t)$.
This two plane parameterization of the light field is the most common one seen in literature, but there are many ways to choose a parameterization.
For instance, one can use a plane and two angles to define each ray passing this plane, which would result in a light field $L(u, v, \theta, \phi)$ where $\theta, \phi \in (0, \pi)$.

\section{Light Field Acquisition}
% Figure epipolar image
% Explain epipolar images: line slope <-> depth

For practical applications, the light field must be discretized and so an appropriate sampling method needs to be chosen.
One way is to capture the light field with a grid of optical systems, e.g. cameras.
Typically, the $(u, v)$-plane is uniformly sampled on a grid $G_{uv} = \left \{ \left( u_i, v_j \right) \mid i = 1,\dots, n, j = 1, \dots, m\right \}$ on the $(u, v)$-plane with a resolution $n \times m$.
The extent in horizontal (vertical) direction is called the horizontal (vertical) \textbf{baseline}.
This means that only a slice of the actual light field can be captured and the two planes are clipped to form a rectangle.

%There are different methods to sample a light field. In principle, every camera can be used to capture 

%The samples on the $(u, v)$-plane correspond to the virtual positions of the camera. 
%Properties of the light field do not change under re-parameterization. 

\subsection*{Oblique Projections}
% Parameterization, Advantages, Disadvantages
% Mention Tomography
% Rotation 

Oblique projection, as shown in figure~\ref{fig:ObliqueProjection}, is a special case of orthographic projection: The parallel rays do not need to be perpendicular to the image plane of the camera.
The advantage is that there is a one-to-one correspondence between camera position and ray angle, since all rays in one camera are parallel.
This means that the angular resolution is simply the number of cameras, and the spatial resolution is the number of pixels in the image plane.
Given a light field $L(u, v, s, t)$ and the distance $d$ between the two planes, a re-parameterization $L^{\prime}(\theta, \phi, s, t)$ can be obtained according to figure~\ref{fig:ObliqueProjectionReparameterization} by the transformation
\begin{align*}
		& \theta = \arctan\left(\frac{u - s}{d}\right), & \phi = \arctan\left(\frac{v - t}{d}\right).
\end{align*}
However, this type of projection is only applicable for synthetic scenes that are rendered with a computer.
% TODO: Refer to a figure with a synthetic scene.


\subsection*{Perspective Projections}
% Figure
% Parameterization, two main types: global or relative
% Reparameterization: Orthographic <-> Perspective
% 

\section{Light Field Tomography}
% Derivation according to Wetzstein
% Explain linearity in log domain
% Mention other tomographic projection types
% SART and ART solvers
%

\section{Spectral Analysis}
% Introduction to fourier transform
% Explain the spectral support of LF



\begin{figure}
	\centering
	\input{../Figures/two-plane-parameterization}
	\caption{Parametrization of the light field with two planes.}
	\label{fig:LightFieldParametrization}
\end{figure}

\begin{figure}
	\subfigure[]{
		\centering
		\input{../Figures/oblique-projection}
		\label{fig:ObliqueProjection}
	}
	\hfill
	\subfigure[]{
		\centering
		\input{../Figures/oblique-projection-reparameterization}
		\label{fig:ObliqueProjectionReparameterization}
	}
	\caption{(a) Light field aquisition using oblique projection. (b) Re-parameterization of the two-plane representation to angular coordinates.}
\end{figure}

\begin{figure}
	\subfigure[]{
		\centering
		\input{../Figures/perspective-projection-shift}
		\label{fig:ShiftedPerspectiveProjection}
	}
	\hfill
	\subfigure[]{
		\centering
		\input{../Figures/perspective-projection-sheared}
		\label{fig:ShearedPerspectiveProjection}
	}
	\caption{Perspective projections of a scene. (a) Shifted Projection (b) Sheared projection.}
\end{figure}


