\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{WETZ_TOMO}
\citation{WETZ_TENS}
\citation{WETZ_TOMO}
\citation{LEVO_LFREN}
\citation{WETZ_TOMO}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:perspective_shifted_projections}{{1a}{2}{Perspective projections, images are shifted\relax }{figure.caption.2}{}}
\newlabel{sub@fig:perspective_shifted_projections}{{a}{2}{Perspective projections, images are shifted\relax }{figure.caption.2}{}}
\newlabel{fig:sheared_projections}{{1b}{2}{Sheared projections, cameras have a common image plane\relax }{figure.caption.2}{}}
\newlabel{sub@fig:sheared_projections}{{b}{2}{Sheared projections, cameras have a common image plane\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Different methods to capture a lightfield\relax }}{2}{figure.caption.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Related work}{2}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Types of light fields}{2}{section.2}}
\newlabel{sec:lftypes}{{2}{2}{Types of light fields}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Notation}{2}{section.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}A first implementation for orthographic projections}{2}{section.4}}
\newlabel{sec:first_implementation}{{4}{2}{A first implementation for orthographic projections}{section.4}{}}
\newlabel{eq:core_problem}{{1}{2}{A first implementation for orthographic projections}{equation.4.1}{}}
\citation{CONV_SART}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The rays captured by camera $i$ using orthographic projection. All rays from one camera have the same angle $\theta _i$, which is measured relative to the surface normal of the layers. $x_6^i$, $s_5^2$ and $s_7^1$ denote the intersections of the ray with the camera sensor and the layers. $d_L$ is the distance between the layers.\relax }}{3}{figure.caption.3}}
\newlabel{fig:orthographic_cameras_layers_sketch}{{2}{3}{The rays captured by camera $i$ using orthographic projection. All rays from one camera have the same angle $\theta _i$, which is measured relative to the surface normal of the layers. $x_6^i$, $s_5^2$ and $s_7^1$ denote the intersections of the ray with the camera sensor and the layers. $d_L$ is the distance between the layers.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Moving to light fields of type 1}{3}{section.5}}
\newlabel{fig:problem_camera_to_layers}{{3a}{4}{Two rays hitting a cameras sensor in neighbouring pixels. The two rays intersect with the layer pixels, with multiple pixels in between.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:problem_camera_to_layers}{{a}{4}{Two rays hitting a cameras sensor in neighbouring pixels. The two rays intersect with the layer pixels, with multiple pixels in between.\relax }{figure.caption.4}{}}
\newlabel{fig:problem_layers_to_camera}{{3b}{4}{Rays coming from four different layer pixels and hitting the same sensor pixel in the camera.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:problem_layers_to_camera}{{b}{4}{Rays coming from four different layer pixels and hitting the same sensor pixel in the camera.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Problems that can arise from different resolution in image- and layer space.\relax }}{4}{figure.caption.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Approach 1: From camera pixels to layer pixels}{4}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Approach 2: Converting the light field}{4}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Approach 3: From layer pixels to camera pixels}{4}{subsection.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Two rays (red) intersecting the first layer (blue) at position $s_i^1$. The rays are captured by different cameras at positions $u_1$ and $u_2$, hitting the camera sensor at locations $x_j^1$ and $x_k^2$. The dotted lines represent the field of view of each camera.\relax }}{5}{figure.caption.5}}
\newlabel{fig:cameras_layers_sketch}{{4}{5}{Two rays (red) intersecting the first layer (blue) at position $s_i^1$. The rays are captured by different cameras at positions $u_1$ and $u_2$, hitting the camera sensor at locations $x_j^1$ and $x_k^2$. The dotted lines represent the field of view of each camera.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Creating synthetic light fields}{5}{section.6}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Review of the two implementations}{6}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Orthographic projections}{6}{subsection.7.1}}
\newlabel{fig:P_orthographic_full}{{5a}{6}{Full matrix.\relax }{figure.caption.6}{}}
\newlabel{sub@fig:P_orthographic_full}{{a}{6}{Full matrix.\relax }{figure.caption.6}{}}
\newlabel{fig:P_orthographic_section}{{5b}{6}{The upper left section of the matrix.\relax }{figure.caption.6}{}}
\newlabel{sub@fig:P_orthographic_section}{{b}{6}{The upper left section of the matrix.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The structure of the propagation matrix $P$. The non-zero elements are marked as blue.\relax }}{6}{figure.caption.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Optimization for three layers. The light field has an angular resolution of $7\times 7$ views. The blur in the reconstruction is also in the original light field.\relax }}{7}{figure.caption.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Perspective projections}{8}{subsection.7.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Optimization for three layers. The light field has an angular resolution of $5\times 5$ views. Parameters: $z = 8$, $d_c = \left ( 0.05, 0.05 \right )$, $\text  {fov} = \left ( 60^\circ , 45^\circ \right )$, $d_L = 1.5$\relax }}{9}{figure.caption.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Optimization for three layers. The light field has an angular resolution of $5\times 5$ views.\relax }}{10}{figure.caption.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Optimization for three layers. The light field has an angular resolution of $9\times 9$ views.\relax }}{11}{figure.caption.10}}
\newlabel{fig:legotruck_artefacts_r=0}{{9}{11}{Optimization for three layers. The light field has an angular resolution of $9\times 9$ views.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Optimization for three layers. The light field has an angular resolution of $9\times 9$ views.\relax }}{12}{figure.caption.11}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Fixing the artefacts that occur in the reconstruction}{13}{section.8}}
\newlabel{fig:intersections_and_weights_overview}{{11a}{13}{Ray (red) coming from a pixel on layer $s^2$ intersecting the sensor plane $x$ and the topmost layer $s^1$. \relax }{figure.caption.12}{}}
\newlabel{sub@fig:intersections_and_weights_overview}{{a}{13}{Ray (red) coming from a pixel on layer $s^2$ intersecting the sensor plane $x$ and the topmost layer $s^1$. \relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Calculation of the intersections and weights.\relax }}{13}{figure.caption.12}}
\newlabel{fig:intersections_and_weights}{{11}{13}{Calculation of the intersections and weights.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Comparison of the three methods implemented so far}{13}{subsection.8.1}}
\newlabel{fig:normalization_no_weights}{{12b}{14}{Row-normalization, box radius $0$, $w \equiv 1$.\relax }{figure.caption.13}{}}
\newlabel{sub@fig:normalization_no_weights}{{b}{14}{Row-normalization, box radius $0$, $w \equiv 1$.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Reconstructions of the central view $\left (2, 2\right )$ from the $3 \times 3$ dice scene with different parameters.\relax }}{14}{figure.caption.13}}
\newlabel{fig:normalization_impact_comparison}{{12}{14}{Reconstructions of the central view $\left (2, 2\right )$ from the $3 \times 3$ dice scene with different parameters.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Interpolation on the sensor plane}{14}{subsection.8.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Comparison of the reconstruction with weights on layers on and off for the legotruck-scene. $9 \times 9$ views, 5 Layers, box-radius 0.\relax }}{15}{figure.caption.14}}
\newlabel{fig:comparison_sensor_interpolation_w=1_vs_w=gauss}{{13}{15}{Comparison of the reconstruction with weights on layers on and off for the legotruck-scene. $9 \times 9$ views, 5 Layers, box-radius 0.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Comparison of two back-projections (top and bottom row) onto five layers (left to right). The first example has the sensor plane at $z_{sensor} = -0.5$, thus zero disparity is mapped onto the middle layer(s). The example below has the sensor plane at $z_{sensor} = -3$. This means that the zero-disparities are mapped onto a layer closer to the observer. The first example is better because it covers the entire depth-range of the scene (the cards in the back are in focus on the first layer and the card in the front is in focus on the 5th layer, not so in the example below).\relax }}{16}{figure.caption.15}}
\newlabel{fig:comparison_of_two_backprojections}{{14}{16}{Comparison of two back-projections (top and bottom row) onto five layers (left to right). The first example has the sensor plane at $z_{sensor} = -0.5$, thus zero disparity is mapped onto the middle layer(s). The example below has the sensor plane at $z_{sensor} = -3$. This means that the zero-disparities are mapped onto a layer closer to the observer. The first example is better because it covers the entire depth-range of the scene (the cards in the back are in focus on the first layer and the card in the front is in focus on the 5th layer, not so in the example below).\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Introduction of the sampling plane}{16}{subsection.8.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Back-Projection and positioning of the layers}{16}{subsection.8.4}}
\newlabel{eq:back_projection}{{2}{16}{Back-Projection and positioning of the layers}{equation.8.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Tile-based optimization for attenuation layers}{16}{subsection.8.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Top row: Tiles without overlap. Bottom row: Tiles with 50\% overlap. The layers have a resolution of $512 \times 512$ pixels and the tiles have $200 \times 200$ pixels. The two examples use $3 \times 3$ tiles and $5 \times 5$ tiles respectively.\relax }}{17}{figure.caption.16}}
\newlabel{fig:comparison_tile_overlap_vs_no_overlap}{{15}{17}{Top row: Tiles without overlap. Bottom row: Tiles with 50\% overlap. The layers have a resolution of $512 \times 512$ pixels and the tiles have $200 \times 200$ pixels. The two examples use $3 \times 3$ tiles and $5 \times 5$ tiles respectively.\relax }{figure.caption.16}{}}
\newlabel{fig:quadratic_blending_mask}{{16a}{17}{\relax }{figure.caption.17}{}}
\newlabel{sub@fig:quadratic_blending_mask}{{a}{17}{\relax }{figure.caption.17}{}}
\newlabel{fig:sum_of_quadratic_blending_masks}{{16b}{17}{\relax }{figure.caption.17}{}}
\newlabel{sub@fig:sum_of_quadratic_blending_masks}{{b}{17}{\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces A quadratic mask (a) is used to blend the overlaps of the tiles. The sum of the masks is shown in (b).\relax }}{17}{figure.caption.17}}
\newlabel{fig:blending_mask_for_tiles}{{16}{17}{A quadratic mask (a) is used to blend the overlaps of the tiles. The sum of the masks is shown in (b).\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Reconstructed views from the attenuation layers created with tiles shown in figure~\ref  {fig:comparison_tile_overlap_vs_no_overlap}.\relax }}{18}{figure.caption.18}}
\newlabel{fig:Reconstructions_from_tiled_attenuator}{{17}{18}{Reconstructed views from the attenuation layers created with tiles shown in figure~\ref {fig:comparison_tile_overlap_vs_no_overlap}.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Mean square error (MSE) images for the reconstructed views in figure~\ref  {fig:Reconstructions_from_tiled_attenuator}. The root mean square error (RMSE) is written besides the angular coordinate.\relax }}{19}{figure.caption.19}}
\newlabel{fig:MSE_for_reconstructions_from_tiled_attenuator}{{18}{19}{Mean square error (MSE) images for the reconstructed views in figure~\ref {fig:Reconstructions_from_tiled_attenuator}. The root mean square error (RMSE) is written besides the angular coordinate.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces The legotruck scene, produced by 5 attenuation layers (a) to (e) using $4 \times 6$ tiles of size $200 \times 200$ with an overlap of 50\%. Below are the reconstructions of the views (f) to (n) from the $6 \times 6$ camera grid (every other left out).\relax }}{20}{figure.caption.20}}
\newlabel{fig:Legotruck_scene_layers and_reconstructions_using_tiles}{{19}{20}{The legotruck scene, produced by 5 attenuation layers (a) to (e) using $4 \times 6$ tiles of size $200 \times 200$ with an overlap of 50\%. Below are the reconstructions of the views (f) to (n) from the $6 \times 6$ camera grid (every other left out).\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Mean square error (MSE) images for the reconstructed views in figure~\ref  {fig:Legotruck_scene_layers and_reconstructions_using_tiles}. The root mean square error (RMSE) is written besides the angular coordinate.\relax }}{21}{figure.caption.21}}
\newlabel{fig:MSE_for_legotruck_scene_reconstruction_with_tiles}{{20}{21}{Mean square error (MSE) images for the reconstructed views in figure~\ref {fig:Legotruck_scene_layers and_reconstructions_using_tiles}. The root mean square error (RMSE) is written besides the angular coordinate.\relax }{figure.caption.21}{}}
\bibstyle{alpha}
\bibdata{lit}
\bibcite{LEVO_LFREN}{LH96}
\bibcite{WETZ_TOMO}{WLHR11}
\bibcite{WETZ_TENS}{WLHR12}
\bibcite{CONV_SART}{Yan10}
